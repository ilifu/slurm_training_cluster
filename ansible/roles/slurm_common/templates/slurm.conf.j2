ClusterName={{ slurm_config.cluster_name }}

SlurmctldHost={{ groups['slurm_controller'][0] | replace('_','-') }}({{ hostvars[groups['slurm_controller'][0]].private_ip }})
#SlurmctldHost=
# 
#DisableRootJobs=NO 
#EnforcePartLimits=NO 
#Epilog=
#EpilogSlurmctld= 
#FirstJobId=1 
#MaxJobId=999999 
#GresTypes= 
#GroupUpdateForce=0 
#GroupUpdateTime=600 
#JobFileAppend=0 
#JobRequeue=1 
#JobSubmitPlugins=1 
#KillOnBadExit=0 
#LaunchType=launch/slurm 
#Licenses=foo*4,bar 
#MailProg=/bin/mail
MailProg=/usr/bin/true
#MaxJobCount=5000 
#MaxStepCount=40000 
#MaxTasksPerNode=128 
MpiDefault=none
#MpiParams=ports=#-# 
#PluginDir= 
#PlugStackConfig= 
#PrivateData=jobs 

ProctrackType=proctrack/cgroup
#ProctrackType=proctrack/linuxproc

#Prolog=
#PrologFlags= 
#PrologSlurmctld= 
#PropagatePrioProcess=0 
#PropagateResourceLimits= 
#PropagateResourceLimitsExcept= 
RebootProgram="/sbin/reboot"
ReturnToService=1
#SallocDefaultCommand= 
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort={{ slurm.ctld_port }}
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort={{ slurm.slurmd_port }}
SlurmdSpoolDir={{ slurm.spool_dir }}
SlurmUser={{ slurm_config.username }}
#SlurmdUser=root 
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurm_state
SwitchType=switch/none
#TaskEpilog=

#TaskPlugin=task/cgroup
#TaskPlugin=task/none
TaskPlugin=task/cgroup

#TaskPluginParam=
#TaskProlog=
#TopologyPlugin=topology/tree 
#TmpFS=/tmp 
#TrackWCKey=no 
#TreeWidth= 
#UnkillableStepProgram= 
#UsePAM=0 
# 
# 
# TIMERS 
#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
#HealthCheckInterval=0 
#HealthCheckProgram= 
InactiveLimit=0
KillWait=30
#MessageTimeout=10 
#ResvOverRun=0 
MinJobAge=3600
#OverTimeLimit=0 
SlurmctldTimeout=120
SlurmdTimeout=60
#UnkillableStepTimeout=60 
#VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
#DefMemPerCPU=0 
# FastSchedule=1
#MaxMemPerCPU=0 
#SchedulerTimeSlice=30 
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_CPU_Memory
# 
# 
# JOB PRIORITY 
#PriorityFlags= 
#PriorityType=priority/basic 
#PriorityDecayHalfLife= 
#PriorityCalcPeriod= 
#PriorityFavorSmall= 
#PriorityMaxAge= 
#PriorityUsageResetPeriod= 
#PriorityWeightAge= 
#PriorityWeightFairshare= 
#PriorityWeightJobSize= 
#PriorityWeightPartition= 
#PriorityWeightQOS= 
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageEnforce=limits,qos
AccountingStorageHost={{ slurm_config.database_host | replace('_','-') }}
#AccountingStorageLoc=
#AccountingStoragePass=
AccountingStoragePort={{ slurm.dbd_port }}
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
#AccountingStoreJobComment=YES
#AccountingStoreFlags=job_comment

#DebugFlags= 
JobCompHost=training_controller
#JobCompLoc=

JobCompType=jobcomp/none
# JobCompPass=
# JobCompPort={# slurm_conf.dbd.port #}
# JobCompType=jobcomp/slurmdbd
# JobCompUser={# slurm_conf.user.name #}

#JobContainerType=job_container/none 
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/cgroup
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmSchedLogFile=/var/log/slurm/slurmsched.log
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
#SuspendProgram= 
#ResumeProgram= 
#SuspendTimeout= 
#ResumeTimeout= 
#ResumeRate= 
#SuspendExcNodes= 
#SuspendExcParts= 
#SuspendRate= 
#SuspendTime= 
# 
# 
# COMPUTE NODES
# NodeName=slurm_worker-00[01-10] CPUs=32 RealMemory=229376 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN
{% for host in groups['slurm_compute'] %}
NodeName={{ host }} CPUs={{ hostvars[host].ansible_processor_vcpus }} RealMemory={{ hostvars[host].ansible_memory_mb.real.free }} Sockets=1 CoresPerSocket={{ hostvars[host].ansible_processor_vcpus }} ThreadsPerCore=1 State=UNKNOWN
{% endfor %}

PartitionName=training Nodes={% for host in groups['slurm_compute'] %}{{ host }}{% if not loop.last %},{% endif %}{% endfor %} Default=YES MaxTime=14-00:00:00 DefMemPerCPU=4096 State=UP

#PrologFlags=x11
#X11Parameters=local_xauthority
