# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a SLURM training cluster deployment toolkit that uses Packer, Terraform, and Ansible to create OpenStack-based computational clusters. The system creates:

- 1 login node (head node) (2 cores, ~8GiB RAM)  
- 1 SLURM controller node (2 cores, ~8GiB RAM)
- 1 database node (2 cores, ~8GiB RAM)
- 1 LDAP node (1 core, ~4GiB RAM)
- 6 compute nodes (240GiB RAM each, `ilifu-G-240G` flavor)
- CephFS shared storage for `/users` (~50GiB), `/software` (~20GiB), and `/data`

## Architecture

### Infrastructure Components
- **Packer**: Builds base VM images with pre-configured software
- **Terraform**: Manages OpenStack infrastructure deployment (networks, instances, storage)
- **Ansible**: Configures deployed nodes with services (SLURM, LDAP, databases)

### Key Directories
- `ansible/`: Ansible playbooks and roles for node configuration
- `templates/`: Terraform template files for configuration generation
- `ansible/roles/`: Service-specific Ansible roles (slurm_*, ldap_*, etc.)

### Deployment Order
Terraform deploys nodes in a specific sequence to optimize resource allocation:
1. **Compute nodes** (6x `ilifu-G-240G`) - Deploy first to claim large hardware
2. **LDAP + Database nodes** - Core services 
3. **Controller node** - SLURM controller (depends on database/LDAP)
4. **Login node** - Entry point (depends on controller)

## Development Commands

### Environment Setup
```bash
# Install dependencies using uv (modern Python package manager)
uv sync

# Or using traditional pip
pip install -r requirements.txt

# Source OpenStack credentials (required before building)
source app-cred-CBIO_Training_August2025-openrc.sh
```

### Building and Deployment
```bash
# Initialize Packer plugins
packer init .

# Build all images and initialize Terraform (automated script)
./build.sh

# Initialize Terraform (if not using build.sh)
terraform init

# Deploy infrastructure
terraform apply

# Configure nodes with Ansible
cd ansible
ansible-playbook -i ../inventory.ini site.yml
```

### Configuration Management
```bash
# Generate variables template from Packer configuration
./create_variables_auto_hcl_template.sh

# Validate Ansible playbooks
ansible-lint ansible/site.yaml

# Test Ansible connectivity
ansible -i inventory.ini all -m ping
```

### Python Development
```bash
# Run the minimal Python module
python main.py

# Run tests (uses pytest per global configuration)
pytest
```

## Configuration Files

- `variables.hcl`: Variable declarations (shared by Packer and Terraform via symlinks)
- `variables.auto.hcl`: Main configuration file with variable values
- `variables.auto.hcl.template`: Template showing all available variables
- `inventory.ini`: Auto-generated by Terraform for Ansible
- `ansible/group_vars/`: Ansible variable files organized by host groups
- `ansible.cfg`: Ansible configuration with connection settings

### Symbolic Links
The following symlinks allow variable sharing between tools:
- `variables.pkr.hcl` → `variables.hcl` (Packer variable declarations)
- `variables.tf` → `variables.hcl` (Terraform variable declarations)  
- `slurm.auto.tfvars` → `variables.auto.hcl` (Terraform auto-loads values)

## Important Notes

- Each training cluster should use a separate repository clone due to local Terraform state management
- The `variables.auto.hcl` file must be created from the template and all `<unknown>` values configured
- OpenStack credentials must be sourced before any Packer/Terraform operations (use `uv run openstack` commands)
- The build process is incremental - existing images won't be rebuilt unless missing
- Packer now uses image names instead of IDs (e.g., `20250728-jammy`)
- Node names can be customized with the `node_name_suffix` variable
- User management is handled through LDAP with the `add_user.py` script on the login node

## Recent Updates

- **Increased compute capacity**: 6 nodes with 240GiB RAM each (`ilifu-G-240G` flavor)
- **Sequential deployment**: Compute nodes deploy first to claim optimal hardware
- **Image name support**: Packer uses image names instead of UUIDs for better maintainability
- **Node naming flexibility**: Added `node_name_suffix` variable for custom node naming
- **Modern Python tooling**: Uses `uv` as the preferred package manager